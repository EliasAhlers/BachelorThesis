%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Malte Schilling at 2024-05-16 18:29:49 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@misc{boettcher2024,
	author = {Stefan B{\"o}ttcher},
	date-added = {2024-05-16 18:29:03 +0200},
	date-modified = {2024-05-16 18:29:49 +0200},
	howpublished = {\url{https://cs.uni-paderborn.de/ecdb/lehre/abschlussarbeiten/hinweise-zum-schreiben-von-bachelor-masterarbeiten/}},
	note = {Accessed on 2024-05-12},
	title = {Hinweise zum Schreiben von Bachelor-/Masterarbeiten},
	year = {2024},
	bdsk-url-1 = {http://cnx.org/contents/GFy_h8cu@10.99:cs_Pb-GW@8/How-Neurons-Communicate}}

@misc{moeckel2024,
	author = {Rolf Moeckel},
	date-added = {2024-05-16 18:28:13 +0200},
	date-modified = {2024-05-16 18:28:59 +0200},
	howpublished = {\url{https://www.mos.ed.tum.de/tb/lehre/theses/structure/}},
	note = {Accessed on 2024-05-11},
	title = {Typical Structure of a Bachelor's or Master's Thesis},
	year = {2024},
	bdsk-url-1 = {http://cnx.org/contents/GFy_h8cu@10.99:cs_Pb-GW@8/How-Neurons-Communicate}}

@misc{paperpile2023methods,
	date-added = {2024-05-16 10:26:28 +0200},
	date-modified = {2024-05-16 10:27:12 +0200},
	howpublished = {\url{https://paperpile.com/g/what-is-research-methodology/}},
	key = {Paperpile LLC},
	note = {Accessed on 2024-05-15},
	title = {What is research methodology?},
	year = {2023},
	bdsk-url-1 = {http://cnx.org/contents/GFy_h8cu@10.99:cs_Pb-GW@8/How-Neurons-Communicate}}

@article{schilling2023,
	abstract = {Modularity as observed in biological systems has proven valuable for guiding classical motor theories towards good answers about action selection and execution. New challenges arise when we turn to learning: Trying to scale current computational models, such as deep reinforcement learning (DRL), to action spaces, input dimensions, and time horizons seen in biological systems still faces severe obstacles unless vast amounts of training data are available. This leads to the question: does biological modularity also hold an important key for better answers to obtain efficient adaptivity for deep reinforcement learning? We review biological experimental work on modularity in biological motor control and link this with current examples of (deep) RL approaches. Analyzing outcomes of simulation studies, we show that these approaches benefit from forms of modularization as found in biological systems. We identify three different strands of modularity exhibited in biological control systems. Two of them---modularity in state (i) and in action (ii) spaces---appear as a consequence of local interconnectivity (as in reflexes) and are often modulated by higher levels in a control hierarchy. A third strand arises from chunking of action elements along a (iii) temporal dimension. Usually interacting in an overarching spatio-temporal hierarchy of the overall system, the three strands offer major ``factors''decomposing the entire modularity structure. We conclude that modularity with its above strands can provide an effective prior for DRL approaches to speed up learning considerably and making learned controllers more robust and adaptive.},
	author = {Schilling, Malte and Hammer, Barbara and Ohl, Frank W. and Ritter, Helge J. and Wiskott, Laurenz},
	date = {2023/01/20},
	date-added = {2024-05-15 20:56:49 +0200},
	date-modified = {2024-05-15 20:56:49 +0200},
	doi = {10.1007/s12559-022-10080-w},
	isbn = {1866-9964},
	journal = {Cognitive Computation},
	title = {Modularity in Nervous Systems---a Key to Efficient Adaptivity for Deep Reinforcement Learning},
	url = {https://doi.org/10.1007/s12559-022-10080-w},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s12559-022-10080-w}}

@article{schilling2021,
	abstract = {Decentralization is a central characteristic of biological motor control that allows for fast responses relying on local sensory information. In contrast, the current trend of Deep Reinforcement Learning (DRL) based approaches to motor control follows a centralized paradigm using a single, holistic controller that has to untangle the whole input information space. This motivates to ask whether decentralization as seen in biological control architectures might also be beneficial for embodied sensori-motor control systems when using DRL. To answer this question, we provide an analysis and comparison of eight control architectures for adaptive locomotion that were derived for a four-legged agent, but with their degree of decentralization varying systematically between the extremes of fully centralized and fully decentralized. Our comparison shows that learning speed is significantly enhanced in distributed architectures---while still reaching the same high performance level of centralized architectures---due to smaller search spaces and local costs providing more focused information for learning. Second, we find an increased robustness of the learning process in the decentralized cases---it is less demanding to hyperparameter selection and less prone to becoming trapped in poor local minima. Finally, when examining generalization to uneven terrains---not used during training---we find best performance for an intermediate architecture that is decentralized, but integrates only local information from both neighboring legs. Together, these findings demonstrate beneficial effects of distributing control into decentralized units and relying on local information. This appears as a promising approach towards more robust DRL and better generalization towards adaptive behavior.},
	author = {Malte Schilling and Andrew Melnik and Frank W. Ohl and Helge J. Ritter and Barbara Hammer},
	date-added = {2024-05-15 20:56:13 +0200},
	date-modified = {2024-05-15 20:56:13 +0200},
	doi = {https://doi.org/10.1016/j.neunet.2021.09.017},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Deep Reinforcement Learning, Motor control, Decentralization, Local information},
	pages = {699-725},
	title = {Decentralized control and local information for robust and adaptive decentralized Deep Reinforcement Learning},
	volume = {144},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608021003671},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2021.09.017}}

@misc{webpage1,
	date-added = {2023-11-14 15:42:04 +0100},
	date-modified = {2023-11-14 15:43:08 +0100},
	howpublished = {\url{http://company.com}},
	key = {CompanyName},
	organization = {Company Name, Ltd.},
	title = {The company's webplatform},
	year = 2013}

@misc{paperpile2023structure2,
	date-added = {2023-11-14 15:17:08 +0100},
	date-modified = {2023-11-14 15:17:17 +0100},
	title = {How to structure a thesis},
	url = {https://paperpile.com/g/thesis-structure/},
	bdsk-url-1 = {http://cnx.org/contents/GFy_h8cu@10.99:cs_Pb-GW@8/How-Neurons-Communicate}}

@misc{paperpile2023structure,
	date-added = {2023-11-14 12:16:44 +0100},
	date-modified = {2023-11-14 15:45:22 +0100},
	howpublished = {\url{https://paperpile.com/g/thesis-structure/}},
	key = {Paperpile LLC},
	note = {Accessed on 2023-11-12},
	title = {How to structure a thesis},
	year = {2023},
	bdsk-url-1 = {http://cnx.org/contents/GFy_h8cu@10.99:cs_Pb-GW@8/How-Neurons-Communicate}}

@book{jele2010,
	author = {Jele, Harald},
	publisher = {R. Oldenbourg Verlag},
	title = {Wissenschaftliches Arbeiten: Zitieren},
	year = {2010}}


% My stuff
@Article{reznicek2010falko,
  author  = {Reznicek, Marc and Walter, Maik and Schmidt, Karin and L{\"u}deling, Anke and Hirschmann, Hagen and Krummes, Cedric and Andreas, Torsten},
  journal = {Institut f{\"u}r deutsche Sprache und Linguistik, Humboldt-Universit{\"a}t zu Berlin, Berlin},
  title   = {{D}as {F}alko-{H}andbuch: {K}orpusaufbau und {A}nnotationen},
  year    = {2010},
}

@Article{LLaMA3,
  author        = {MetaAI},
  title         = {{T}he {L}lama 3 {H}erd of {M}odels},
  year          = {2024},
  month         = jul,
  abstract      = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2407.21783},
  eprint        = {2407.21783},
  file          = {:Dubey2024 - The Llama 3 Herd of Models.pdf:PDF:http\://arxiv.org/pdf/2407.21783v2},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@InProceedings{boyd2014merlin,
  author       = {Boyd, Adriane and Hana, Jirka and Nicolas, Lionel and Meurers, Detmar and Wisniewski, Katrin and Abel, Andrea and Sch{\"o}ne, Karin and Stindlov{\'a}, Barbora and Vettori, Chiara},
  booktitle    = {LREC},
  title        = {The {M}ERLIN corpus: {L}earner {L}anguage and the {CEFR}.},
  year         = {2014},
  organization = {Reykjavik, Iceland},
  pages        = {1281--1288},
}

@Misc{claude3.5sonnet,
  author       = {{Anthropic}},
  howpublished = {\url{https://www.anthropic.com}},
  note         = {Large language model},
  title        = {{C}laude 3.5 {S}onnet},
  year         = {2024},
}

@InProceedings{Siddhant2020,
  author  = {Aditya Siddhant and Junjie Hu and Melvin Johnson and Orhan Firat and Sebastian Ruder},
  title   = {{XTREME}: A {M}assively {M}ultilingual {M}ulti-task {B}enchmark for {E}valuating {C}ross-lingual {G}eneralization},
  year    = {2020},
  comment = {This seems to show, that LLMs tend to be better in task in the training dominant languag},
}

@Article{Pires2019,
  author        = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  title         = {How multilingual is {M}ultilingual {BERT}?},
  year          = {2019},
  month         = jun,
  abstract      = {In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.},
  archiveprefix = {arXiv},
  comment       = {suggests that performance might be better when the instruction language and the task language are more similar},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1906.01502},
  eprint        = {1906.01502},
  file          = {:Pires2019 - How Multilingual Is Multilingual BERT_.pdf:PDF:http\://arxiv.org/pdf/1906.01502v1},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Jiang2023,
  author        = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
  title         = {{M}istral 7{B}},
  year          = {2023},
  month         = oct,
  abstract      = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2310.06825},
  eprint        = {2310.06825},
  file          = {:Jiang2023 - Mistral 7B.pdf:PDF:http\://arxiv.org/pdf/2310.06825v1},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Abdin2024,
  author        = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, Sébastien and Cai, Qin and Cai, Martin and Mendes, Caio César Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Yen-Chun and Chen, Yi-Ling and Chopra, Parul and Dai, Xiyang and Del Giorno, Allie and de Rosa, Gustavo and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Iter, Dan and Gao, Mei and Gao, Min and Gao, Jianfeng and Garg, Amit and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Liu, Ce and Liu, Mengchen and Liu, Weishung and Lin, Eric and Lin, Zeqi and Luo, Chong and Madan, Piyush and Mazzola, Matt and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Wang, Xin and Wang, Lijuan and Wang, Chunyu and Wang, Yu and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wu, Haiping and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Ziyi and Yang, Yifan and Yu, Donghan and Yuan, Lu and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
  title         = {{P}hi-3 {T}echnical {R}eport: A {H}ighly {C}apable {L}anguage {M}odel {L}ocally on {Y}our {P}hone},
  year          = {2024},
  month         = apr,
  abstract      = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench). Moreover, we also introduce phi-3-vision, a 4.2 billion parameter model based on phi-3-mini with strong reasoning capabilities for image and text prompts.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2404.14219},
  eprint        = {2404.14219},
  file          = {:Abdin2024 - Phi 3 Technical Report_ a Highly Capable Language Model Locally on Your Phone.pdf:PDF:http\://arxiv.org/pdf/2404.14219v3},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{GemmaTeam2024,
  author        = {{Gemma Team} and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivière, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Hussenot, Léonard and Sessa, Pier Giuseppe and Chowdhery, Aakanksha and Roberts, Adam and Barua, Aditya and Botev, Alex and Castro-Ros, Alex and Slone, Ambrose and Héliou, Amélie and Tacchetti, Andrea and Bulanova, Anna and Paterson, Antonia and Tsai, Beth and Shahriari, Bobak and Lan, Charline Le and Choquette-Choo, Christopher A. and Crepy, Clément and Cer, Daniel and Ippolito, Daphne and Reid, David and Buchatskaya, Elena and Ni, Eric and Noland, Eric and Yan, Geng and Tucker, George and Muraru, George-Christian and Rozhdestvenskiy, Grigory and Michalewski, Henryk and Tenney, Ian and Grishchenko, Ivan and Austin, Jacob and Keeling, James and Labanowski, Jane and Lespiau, Jean-Baptiste and Stanway, Jeff and Brennan, Jenny and Chen, Jeremy and Ferret, Johan and Chiu, Justin and Mao-Jones, Justin and Lee, Katherine and Yu, Kathy and Millican, Katie and Sjoesund, Lars Lowe and Lee, Lisa and Dixon, Lucas and Reid, Machel and Mikuła, Maciej and Wirth, Mateo and Sharman, Michael and Chinaev, Nikolai and Thain, Nithum and Bachem, Olivier and Chang, Oscar and Wahltinez, Oscar and Bailey, Paige and Michel, Paul and Yotov, Petko and Chaabouni, Rahma and Comanescu, Ramona and Jana, Reena and Anil, Rohan and McIlroy, Ross and Liu, Ruibo and Mullins, Ryan and Smith, Samuel L and Borgeaud, Sebastian and Girgin, Sertan and Douglas, Sholto and Pandya, Shree and Shakeri, Siamak and De, Soham and Klimenko, Ted and Hennigan, Tom and Feinberg, Vlad and Stokowiec, Wojciech and Chen, Yu-hui and Ahmed, Zafarali and Gong, Zhitao and Warkentin, Tris and Peran, Ludovic and Giang, Minh and Farabet, Clément and Vinyals, Oriol and Dean, Jeff and Kavukcuoglu, Koray and Hassabis, Demis and Ghahramani, Zoubin and Eck, Douglas and Barral, Joelle and Pereira, Fernando and Collins, Eli and Joulin, Armand and Fiedel, Noah and Senter, Evan and Andreev, Alek and Kenealy, Kathleen},
  title         = {{G}emma: {O}pen {M}odels {B}ased on {G}emini {R}esearch and {T}echnology},
  year          = {2024},
  month         = mar,
  abstract      = {This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2403.08295},
  eprint        = {2403.08295},
  file          = {:GemmaTeam2024 - Gemma_ Open Models Based on Gemini Research and Technology.pdf:PDF:http\://arxiv.org/pdf/2403.08295v4},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Loshchilov2017,
  author        = {Loshchilov, Ilya and Hutter, Frank},
  title         = {{D}ecoupled {W}eight {D}ecay {R}egularization},
  year          = {2017},
  month         = nov,
  abstract      = {L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1711.05101},
  eprint        = {1711.05101},
  file          = {:Loshchilov2017 - Decoupled Weight Decay Regularization.pdf:PDF:http\://arxiv.org/pdf/1711.05101v3},
  keywords      = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Mathematics},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Brown2020,
  author        = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  title         = {{L}anguage {M}odels are {F}ew-{S}hot {L}earners},
  year          = {2020},
  month         = may,
  abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2005.14165},
  eprint        = {2005.14165},
  file          = {:Brown2020 - Language Models Are Few Shot Learners.pdf:PDF:http\://arxiv.org/pdf/2005.14165v4},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{unsloth2024,
  author       = {Daniel Han, Michael Han},
  howpublished = {\url{https://github.com/unslothai/unsloth}},
  note         = {Accessed on 21.08.2024},
  title        = {{U}nsloth: {E}fficient {F}ine-tuning for {L}arge {L}anguage {M}odels.},
  year         = {2024},
}

@Article{Paszke2019,
  author        = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  title         = {{P}y{T}orch: An {I}mperative {S}tyle, {H}igh-{P}erformance {D}eep {L}earning {L}ibrary},
  year          = {2019},
  month         = dec,
  abstract      = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1912.01703},
  eprint        = {1912.01703},
  file          = {:Paszke2019 - PyTorch_ an Imperative Style, High Performance Deep Learning Library.pdf:PDF:http\://arxiv.org/pdf/1912.01703v1},
  keywords      = {Machine Learning (cs.LG), Mathematical Software (cs.MS), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Huang2024,
  author        = {Huang, Zhen and Wang, Zengzhi and Xia, Shijie and Liu, Pengfei},
  title         = {{O}lympic{A}rena {M}edal {R}anks: {W}ho {I}s the {M}ost {I}ntelligent {AI} {S}o {F}ar?},
  year          = {2024},
  month         = jun,
  abstract      = {In this report, we pose the following question: Who is the most intelligent AI model to date, as measured by the OlympicArena (an Olympic-level, multi-discipline, multi-modal benchmark for superintelligent AI)? We specifically focus on the most recently released models: Claude-3.5-Sonnet, Gemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic medal Table approach to rank AI models based on their comprehensive performance across various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet shows highly competitive overall performance over GPT-4o, even surpassing GPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2) Gemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and Claude-3.5-Sonnet, but with a clear performance gap between them. (3) The performance of AI models from the open-source community significantly lags behind these proprietary models. (4) The performance of these models on this benchmark has been less than satisfactory, indicating that we still have a long way to go before achieving superintelligence. We remain committed to continuously tracking and evaluating the performance of the latest powerful models on this benchmark (available at https://github.com/GAIR-NLP/OlympicArena).},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2406.16772},
  eprint        = {2406.16772},
  file          = {:Huang2024 - OlympicArena Medal Ranks_ Who Is the Most Intelligent AI so Far_.pdf:PDF:http\://arxiv.org/pdf/2406.16772v2},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{goetheCefr,
  author       = {GoetheInstitut},
  howpublished = {\url{https://www.goethe.de/de/spr/kur/stu.html}},
  note         = {Accessed on 2024-09-09},
  title        = {{DEUTSCHKURSE} und {PRÜFUNGEN}},
}

@Article{Yang2024,
  author        = {Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Dong, Guanting and Wei, Haoran and Lin, Huan and Tang, Jialong and Wang, Jialin and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Ma, Jianxin and Yang, Jianxin and Xu, Jin and Zhou, Jingren and Bai, Jinze and He, Jinzheng and Lin, Junyang and Dang, Kai and Lu, Keming and Chen, Keqin and Yang, Kexin and Li, Mei and Xue, Mingfeng and Ni, Na and Zhang, Pei and Wang, Peng and Peng, Ru and Men, Rui and Gao, Ruize and Lin, Runji and Wang, Shijie and Bai, Shuai and Tan, Sinan and Zhu, Tianhang and Li, Tianhao and Liu, Tianyu and Ge, Wenbin and Deng, Xiaodong and Zhou, Xiaohuan and Ren, Xingzhang and Zhang, Xinyu and Wei, Xipin and Ren, Xuancheng and Liu, Xuejing and Fan, Yang and Yao, Yang and Zhang, Yichang and Wan, Yu and Chu, Yunfei and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Guo, Zhifang and Fan, Zhihao},
  title         = {{Q}wen2 {T}echnical {R}eport},
  year          = {2024},
  month         = jul,
  abstract      = {This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2407.10671},
  eprint        = {2407.10671},
  file          = {:Yang2024 - Qwen2 Technical Report.pdf:PDF:http\://arxiv.org/pdf/2407.10671v3},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Szuuegyi2019,
  author  = {Edit Szuügyi and Soören Etler and Andrew Beaton and Manfred Stede},
  journal = {KONVENS 2019},
  title   = {Automated Assessment of {L}anguage {P}roficiency on {G}erman {D}ata},
  year    = {2019},
}

@Article{Vajjala2018,
  author        = {Vajjala, Sowmya and Rama, Taraka},
  title         = {{E}xperiments with {U}niversal {CEFR} {C}lassification},
  year          = {2018},
  month         = apr,
  abstract      = {The Common European Framework of Reference (CEFR) guidelines describe language proficiency of learners on a scale of 6 levels. While the description of CEFR guidelines is generic across languages, the development of automated proficiency classification systems for different languages follow different approaches. In this paper, we explore universal CEFR classification using domain-specific and domain-agnostic, theory-guided as well as data-driven features. We report the results of our preliminary experiments in monolingual, cross-lingual, and multilingual classification with three languages: German, Czech, and Italian. Our results show that both monolingual and multilingual models achieve similar performance, and cross-lingual classification yields lower, but comparable results to monolingual classification.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1804.06636},
  eprint        = {1804.06636},
  file          = {:Vajjala2018 - Experiments with Universal CEFR Classification.pdf:PDF:http\://arxiv.org/pdf/1804.06636v1},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{Caines2020,
  author    = {Caines, Andrew and Buttery, Paula},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  title     = {{REPROLANG} 2020: Automatic Proficiency Scoring of {C}zech, {E}nglish, {G}erman, {I}talian, and {S}panish Learner Essays},
  year      = {2020},
  address   = {Marseille, France},
  editor    = {Calzolari, Nicoletta and B{\'e}chet, Fr{\'e}d{\'e}ric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  month     = may,
  pages     = {5614--5623},
  publisher = {European Language Resources Association},
  abstract  = {We report on our attempts to reproduce the work described in Vajjala {\&} Rama 2018, {`}Experiments with universal CEFR classification{'}, as part of REPROLANG 2020: this involves featured-based and neural approaches to essay scoring in Czech, German and Italian. Our results are broadly in line with those from the original paper, with some differences due to the stochastic nature of machine learning and programming language used. We correct an error in the reported metrics, introduce new baselines, apply the experiments to English and Spanish corpora, and generate adversarial data to test classifier robustness. We conclude that feature-based approaches perform better than neural network classifiers for text datasets of this size, though neural network modifications do bring performance closer to the best feature-based models.},
  isbn      = {979-10-95546-34-4},
  language  = {English},
  url       = {https://aclanthology.org/2020.lrec-1.689},
}

@Article{Zhao2023,
  author        = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  title         = {A {S}urvey of {L}arge {L}anguage {M}odels},
  year          = {2023},
  month         = mar,
  abstract      = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2303.18223},
  eprint        = {2303.18223},
  file          = {:Zhao2023 - A Survey of Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2303.18223v13},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Vaswani2017,
  author  = {Vaswani, A},
  journal = {Advances in Neural Information Processing Systems},
  title   = {{A}ttention is all you need},
  year    = {2017},
}

@Article{arXiv:2106.09685,
  author    = {Gong, Youdi and Liu, Guangzhen and Xue, Yunzhi and Li, Rui and Meng, Lingzhong},
  journal   = {Information and Software Technology},
  title     = {A survey on dataset quality in machine learning},
  year      = {2023},
  pages     = {107268},
  volume    = {162},
  publisher = {Elsevier},
}

@Article{Hu2021,
  author        = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title         = {{L}o{RA}: {L}ow-{R}ank {A}daptation of {L}arge {L}anguage {M}odels},
  year          = {2021},
  month         = jun,
  abstract      = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2106.09685},
  eprint        = {2106.09685},
  file          = {:Hu2021 - LoRA_ Low Rank Adaptation of Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2106.09685v2},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{LLamaTokenizerPlayground,
  author       = {Belladore},
  howpublished = {\url{https://belladoreai.github.io/llama3-tokenizer-js/example-demo/build/}},
  note         = {Accessed on 2024-09-09},
  title        = {llama3-tokenizer-js-playground},
  year         = {2024},
}

@Article{Devlin2018,
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title         = {{BERT}: Pre-training of {D}eep {B}idirectional {T}ransformers for {L}anguage {U}nderstanding},
  year          = {2018},
  month         = oct,
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1810.04805},
  eprint        = {1810.04805},
  file          = {:Devlin2018 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF:http\://arxiv.org/pdf/1810.04805v2},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Raffel2019,
  author        = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  title         = {{E}xploring the {L}imits of {T}ransfer {L}earning with a {U}nified {T}ext-to-{T}ext {T}ransformer},
  year          = {2019},
  month         = oct,
  abstract      = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1910.10683},
  eprint        = {1910.10683},
  file          = {:Raffel2019 - Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer.pdf:PDF:http\://arxiv.org/pdf/1910.10683v4},
  keywords      = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Comment{jabref-meta: databaseType:bibtex;}
